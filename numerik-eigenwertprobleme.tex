\documentclass[%
a4paper,
%empty,		% keine Seitenzahlen
%a5paper,	% alle weiteren Papierformat einstellbar
11pt,		% Schriftgröße (12pt, 11pt (Standard))
leqno,		% Nummerierung von Gleichungen links
%fleqn,		% Ausgabe von Gleichungen linksbündig
]
{scrartcl}

%% Deutsche Anpassungen 
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

%obligatorischer Mathekram:
\usepackage{amssymb,amstext,dsfont,trsym,pifont}
\usepackage[sumlimits]{amsmath}
\usepackage{eulervm}

%nützlicher Mathekram:
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

%nützliche Extras:
\usepackage{array,
hhline,
longtable,
tabularx,
enumerate,
%hyperref,
color,
setspace,
booktabs,
%cite,
caption,
lineno,
%lastpage,
%algorithm,
ulem,
}
\usepackage{arydshln}

\usepackage[amsmath,thmmarks,thref]{ntheorem}

% meine Theoremdefinitionen:
% +------------------------+

% Definitionen:
\theoremstyle{plain}
\theoremheaderfont{\bfseries}
\theorembodyfont{}
\theoremseparator{\ }
%\theoremprework{\hfill \rule{0.5\textwidth}{1pt} \hspace*{0.25\textwidth} }
%\theorempostwork{\rule}
%\theoremindent2ex
\newtheorem{mydef}{Definition}[section]

%Sätze:
\theoremstyle{plain}
\theoremheaderfont{\bfseries}
\theorembodyfont{}
\theoremsymbol{$\Box$}
\newtheorem{mysatz}[mydef]{Satz}

%Bemerkungen:
\theoremstyle{plain}
\theoremheaderfont{\itshape}
\theorembodyfont{}
\theoremsymbol{}
\newtheorem{mybem}[mydef]{Bemerkung}

%Beispiele:
\theoremstyle{plain}
\theoremheaderfont{\itshape}
\theorembodyfont{}
\theoremsymbol{}
\newtheorem{mybsp}[mydef]{Beispiel}

% Randverwaltung (entweder geometry oder =fullpage=)
%\usepackage[left=1cm,right=1cm,top=1cm,bottom=1cm,includeheadfoot]{geometry}
\usepackage[cm,
%headings,
]{fullpage}

% die fancy-Header:
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}

\fancyhead{}
\fancyfoot{}
%\fancyhead[L]{}
%\fancyhead[C]{}
%\fancyhead[R]{\nouppercase \leftmark}
%\fancyfoot[L]{}
\fancyfoot[C]{\thepage}
%\fancyfoot[OR]{\thepage}
%\fancyfoot[LE]{\thepage}
%Linie oben/unten
\renewcommand{\headrulewidth}{0.0pt}
\renewcommand{\footrulewidth}{0.0pt}

%kein Einrücken der Paragraphen
\parindent 0pt

%% Packages für Grafiken & Abbildungen
%\usepackage{graphicx}
%\usepackage{subfig}    %%Teilabbildungen in einer Abbildung
%\usepackage{tikz}      %%TeX ist kein Zeichenprogramm
%\usepackage[all]{xy}
\usepackage{pst-all}

\begin{document}

\setcounter{section}{6}

\section{Eigenwertprobleme}
\label{sec:Eigenwertprobleme}

\textbf{Motivation:} Mechanische Eigenschwingungen, z.B. Saite oder Membran.

Bewegungsgleichung ist die Wellengleichung mit der Auslenkung $u(x,t)$ an der Stelle $x$ zum Zeitpunkt $t$.\\

1-D Saite:
\[
\frac{\partial^2 u}{\partial t^2} = C \cdot \frac{\partial^2 u}{\partial x^2}
\]
2-D Membran:
\[
\frac{\partial^2 u}{\partial t^2} = C \triangle u - C \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right)
\]
jeweils mit Randberechnung wie $\left. u \right|_{\text{Rand}}=0$.\\

\textbf{Diskretisierung:}
\begin{align*}
  y''(t) & = -Ay(t) \qquad\qquad  A \in \mathbb{R}^{n \times n} \text{ spd., } y(t) \in \mathbb{R}^n\\
  \intertext{\textbf{stationäre Lösung:}}
  \text{Ansatz: } y(t) & = e^{i \omega t} y \qquad \qquad y \in \mathbb{R}^n, \omega \text{ Eigenfrequenz}\\
  \Rightarrow e^{i \omega t} (i \omega)^2 y & = -A e^{i \omega t} y\\
  \Rightarrow Ay & = \omega^2 y = \lambda y \text{ mit } \lambda = \omega^2
\end{align*}

\subsection{Grundlagen} % (fold)
\label{sub:Grundlagen}

\begin{mydef}
Sei $A \in \mathbb{C}^{n \times n}, x \in \mathbb{C}^n \backslash \left\{ 0 \right\}, \lambda \in \mathbb{C}$ und $Ax = \lambda x$. Dann heißt $x$ ein (Rechts-)\textbf{Eigenvektor} (EV) von $A$ zum \textbf{Eigenwert} (EW) $\lambda$ von $A$. Das Paar $(\lambda, x)$ heißt ein Eigenpaar von $A$. Die Menge aller Eigenwerte von $A$
\[
\sigma(A) := \left\{ \lambda \ | \ \lambda \text{ ist Eigenwert von } A \right\}
\]
heißt das \textbf{Spektrum} von $A$.
\end{mydef}
\textit{Bemerkung:}\\[-1.3cm] %rrrrreal dirrty (but who cares anyway when it looks good?!)
\begin{align*}
Ax = \lambda x & \Leftrightarrow (\lambda I - A)x = 0 \\
 & \Leftrightarrow \det(\lambda I - A)=0\\
 & \Leftrightarrow \lambda \text{ ist Nullstelle des charakteristischen Polynoms.}
\end{align*}
Ist $x$ ein Eigenvektor, so auch $\theta x$ mit $\theta \neq 0$. Deshalb werden Eigenvektoren oft normiert, d.h. $\| x \|_2=1$.\\

\begin{mysatz}
Das charakteristische Polynom $p_A(\lambda) = \det(\lambda I - A)$ von $A$ besitzt die eindeutige Faktorisierung
\[
p_A(\lambda) = (\lambda - \lambda_1)^{m_1} \cdot (\lambda - \lambda_2)^{m_2} \cdot \ldots \cdot (\lambda - \lambda_k)^{n_k}
\]
mit den $k$ paarweise verschiedenen Eigenwerten $\lambda_k$ von $A$ mit $m_1 + \ldots + m_k = n$.

Dabei ist $m_i$ die algebraische Vielfachheit von $\lambda_i$.\newline

\textbf{Beweis:} Fundamentalsatz der Algebra.
\end{mysatz}

\begin{mydef}
  Die \textbf{geometrische Vielfachheit} eines Eigenwertes $\lambda$ von $A$ ist die Dimension des \textbf{Nullraumes} von $\lambda I -A$.
\end{mydef}

\newpage

\begin{mybem}
Die geometrische Vielfachheit ist stets kleiner oder gleich der algebraischen Vielfachheit.
So besitzt der \textbf{Jordan-Block}
\[
J_2(0) = 
\begin{pmatrix}
0 & 1\\
0 & 0
\end{pmatrix}
\]
das charakteristische Polynom $\lambda^2$, so dass 0 ein Eigenwert der algebraischen Vielfachheit 2 ist. Jedoch wird der Nullraum von $OI - J_2(0) = \begin{pmatrix}
0 & -1\\
0 & 0
\end{pmatrix}$
vom Einheitsvektor $e_1$ ausgespannt.
Die geometrische Vielfachheit des Eigenwertes 0 ist also nur 1.

Allgemein gilt: Jordan-Blöcke der Dimension größer 1 besitzen kein vollständiges System aus Eigenvektoren (die Matrix besitzt einen \textit{Defekt}).
Die Jordansche Normalform lässt sich numerisch nicht berechnen, denn die Jordan-Blöcke reagieren extrem empfindlich auf Störungen (siehe Beispiel 7.7) %TODO Referenz fixen
\end{mybem}

\textbf{Erinnerung:}

Seien $A,X \in \mathbb{R}^{n \times n}$ und sei $X$ nicht-singulär. Dann ist $B=X^{-1}AX$ \textbf{ähnlich} zu $A$, die Abbildung heißt \textbf{Ähnlichkeitstransformation}.

Wir suchen Ähnlichkeitstransformationen der Gestalt
\[
X^{-1}AX = \Lambda
\]
die $A$ auf Diagonalgestalt transformiert.

Die Spalten von $X$ sind dann die Eigenvektoren, die Diagonalelemente von $\Lambda$ die Eigenwerte.

Besonders vorteilhaft sind Ähnlichkeitstransformationen durch unitäre (orthogonale) Matrizen. Eine unitäre $n \times n$ Matrix besitzt $n^2$ Matrixelemente, die jedoch $\frac{n(n-1)}{2}$ Orthogonalitätsbedingungen und $n$ Normierungsbedingungen erfüllen.
Dann verbleiben $n^2 - \frac{n(n-1)}{2} -n = \frac{n(n-1)}{2}$ Freiheitsgrade. 

Durch geeignete Nutzung der Freiheitsgrade erscheint es möglich, eine gegebene $n \times n$ Matrix auf Dreiecksgestalt zu transformieren, denn eine Dreiecksmatrix besitzt $\frac{n(n-1)}{2}$ Nichtnullelemente.  

\begin{mysatz}
(Schursche Normalform, Schur 1909)

Zu jeder komplexen Matrix $A \in \mathbb{C}^{n \times n}$ existiert eine unitäre Matrix $U \in \mathbb{C}^{n \times n}$, eine Diagonalmatrix 
\[
D = \text{diag}(\lambda_1, \ldots, \lambda_n)
\]
und eine (echte) obere Dreiecksmatrix mit verschwindenen Diagonalelementen, so dass 
\begin{align*}
 & U^H A U = D + N \\
 \text{oder} \qquad & A = U(D+N)U^H
\end{align*}
(Dabei ist $U^H = \overline{U}^T$, die zu $U$ \uline{adjungierte} Matrix; $U$ unitär: $\Leftrightarrow U^HU = I$.)\\

\textit{Bemerkung:}
\begin{enumerate}
    \item Der Satz von Schur ist die natürliche Verallgemeinerung des Spektralsatzes für symmetrische Matrizen; Beweis ebenso induktiv.
    \item Die Diagonalelemente von $D$ sind die Eigenwerte von $A$.
\end{enumerate}
\textbf{Beweis:} Induktionsanfang:

Gemäß dem Fundamentalsatz der Algebra besitzt $A \in \mathbb{C}^{n \times n}$ einen Eigenwert $\lambda_1 \in \mathbb{C}$.

Es gilt $Au = \lambda_1 u$ für ein $u \in \mathbb{C}^n$ mit $\| u \|_2 = 1$.

Ergänze $u$ durch $u_2, \ldots, u_n$ zu einer Orthogonalbasis des $\mathbb{C}^n$ und sei
\begin{align*}
U_1 & = \left[ u, u_2, \ldots, u_n \right] \in \mathbb{C}^{n \times n}
\intertext{Dann gilt}
AU_1 & = \left( \lambda_1 u, Au_2, \ldots, Au_n \right)
\intertext{und weiter}
U_1^H A U_1 & =
\left(
\begin{array}{c:ccc}
\lambda_1 & * & \dots & *\\\hdashline
0 & * & \dots & *\\
\vdots & \vdots & & \vdots\\
0 & * & \dots &  *
\end{array}
\right)
\begin{array}{c}
1\\
\\
n-1\\
\\
\end{array}\\
& \phantom{=} 
\begin{array}{ccccc}
  & \hspace{0.8em} 1 & & n-1 & 
\end{array}
\end{align*}
Induktionsschluss: Argumentiere wie oben auf dem rechten unteren $(n-1) \times (n-1)$ Block

\begin{center}
%+-+-+-+-+
% Jap, das wird als Bild eingefügt, da man sonst nicht so eine schöne Box um die rechte untere
% Teilmatrix malen kann. So sieht's aber echt schnieke aus :)
%+-+-+-+-+
\begin{pspicture}(10,4.5)

% zum justieren:
%\psgrid(0,0)(10,5)

\put(0,2.5){
$
U_i^H \dots U_1^H A U_1 \dots U_i =
\begin{pmatrix}
\lambda_1   & *         & \cdots    & \cdots    & \cdots    & *\\
0           & \ddots    & \ddots    &           &           & \vdots\\
\vdots      & \ddots    & \lambda_i & *         & \cdots    & *\\
\vdots      &           & 0         & *         & \cdots    & *\\
\vdots      &           & \vdots    & \vdots    &           & \vdots\\
0           & \cdots    & 0         & *         & \cdots    & *
\end{pmatrix}
$
}
% Die Box um die Teilmatrix:
\psframe(6.9,2.3)(8.7,0.85)
\end{pspicture}
\end{center}
mit der Blockmatrix
\begin{align*}
U_i & = 
\begin{pmatrix}
1 				& 			&	& \text{0}\\
 				& \ddots 	& 	& \\
 				&			& 1\\
\text{0} 	& 			&  	& \Large\boxed{\^U_i} 
\end{pmatrix}
\qquad
\^U_i \text{ unitär}
\end{align*}
\end{mysatz}

Für normale Matrizen lässt sich mehr zeigen:
\[
A \text{ normal} \Leftrightarrow A^HA = AA^H
\]
Zu den normalen Matrizen zählen die \textbf{hermiteschen/selbstadjungierten} Matrizen (d.h. $A^H = A$) und die reellen symmetrischen Matrizen.

\begin{mysatz}
(Hermite 1850 - \textit{Spektralsatz})

Ist $A \in \C^{n \times n}$ normal, so gibt es eine unitäre Matrix $U$ mit
\[
U^H A U = 
\begin{pmatrix}
\lambda_1 & & 0\\
& \ddots & \\
0 & & \lambda_n
\end{pmatrix}
=
\text{diag}(\lambda_1,\dots,\lambda_n)
\]
\textbf{Beweis:}

Schursche Normalform
\begin{align*}
U^H A U = 
\begin{pmatrix}
\lambda_1 & & *\\
& \ddots & \\
0 & & \lambda_n
\end{pmatrix}
=: R
\end{align*}
Mit $A$ ist auch $R$ normal, denn
\begin{align*}
RR^H & = U^H A \underbrace{(U \cdot U^H)}_{I} A^H U = U^H A^H A U\\
 & = (U^H A^H U) U^H A U  = R^H R
\end{align*}
In Komponenten
\begin{align*}
R R^H & = 
\begin{pmatrix}
\lambda_1   & r_{12}    & \dots     & r_{1n}\\
            & \ddots    & \ddots    & \vdots\\
            &           & \ddots    & r_{n-1,n}\\
0           &           &           & \lambda_n
\end{pmatrix}
\begin{pmatrix}
\overline{\lambda_1}    & & & 0\\
\overline{r_{12}}       & \ddots\\
\vdots                  & \ddots & \ddots\\
\overline{r_{1n}}       & \dots & \dots & \overline{\lambda_n}
\end{pmatrix}\\
& = 
\begin{pmatrix}
\overline{\lambda_1}    & & & 0\\
\overline{r_{12}}       & \ddots\\
\vdots                  & \ddots & \ddots\\
\overline{r_{1n}}       & \dots & \dots & \overline{\lambda_n}
\end{pmatrix}
\begin{pmatrix}
\lambda_1   & r_{12}    & \dots     & r_{1n}\\
            & \ddots    & \ddots    & \vdots\\
            &           & \ddots    & r_{n-1,n}\\
0           &           &           & \lambda_n
\end{pmatrix}
= R^H R
\end{align*}
Also gilt komponentenweise:
\begin{align*}
\text{für } i=j=1: \qquad & |\lambda_1|^2 + |r_{12}|^2 + \dots + |r_{1n}|^2 = |\lambda_1|^2\\
& \Rightarrow r_{12} = r_{13} = \dots = r_{1n} = 0\\
\text{für } i=j=2: \qquad & |\lambda_2|^2 + |r_{23}|^2 + \dots + |r_{2n}|^2 = |\lambda_2|^2\\
& \Rightarrow r_{23} = \dots = r_{2n} = 0 \qquad \text{ usw. usf.}
\end{align*}
\end{mysatz}

% subsection Grundlagen (end)

\subsection{Störungsempfindlichkeit des Eigenwertproblems} % (fold)
\label{sub:Störungsempfindlichkeit des Eigenwertproblems}

Die Störungsempfindlichkeit der Lösung eines LGS hängt wesentlich von der Konditionszahl
\begin{align*}
  \mathcal{K}(A) & = \|A\| \cdot \|A^{-1}\| \qquad \text{ab.}
\end{align*}
Für Eigenwertprobleme sind die Zusammenhänge komplexer, denn die Diagonalisierbarkeit (Fehlen eines Defekts) ist entscheidend.
\begin{mybsp}\label{bspEWJordanBlock} Der Jordan-Block 
$A = 
\begin{pmatrix}
2 & 1\\
0 & 2
\end{pmatrix}
$
ist nicht diagonalisierbar. 

Für $\varepsilon > 0$ sei
$A(\varepsilon) = 
\begin{pmatrix}
2 & 1\\
\varepsilon & 2
\end{pmatrix}
$
mit den Eigenwerten $\lambda_1(\varepsilon)=2+\sqrt{2},\ \lambda_2(\varepsilon)=2-\sqrt{2}$.

Es gilt
\begin{align*}
\frac{d\lambda_1(\varepsilon)}{d\varepsilon} & = \frac{1}{2\sqrt{\varepsilon}} \qquad \frac{d\lambda_2(\varepsilon)}{d\varepsilon} = -\frac{1}{2\sqrt{\varepsilon}}
\end{align*}
also
\begin{align*}
\lim\limits_{\varepsilon \rightarrow 0} & \left| \frac{d \lambda_i(\varepsilon)}{d \varepsilon} \right| = \infty
\end{align*}
\center $\Rightarrow$ \textbf{unbeschränkte Störungsempfindlichkeit!}
\end{mybsp}

\newpage

%TODO: ref fixen!
Beispiel~\ref{bspEWJordanBlock} zeigt, dass Eigenwerte nicht stetig diff'bar von den Matrixelementen abhängen können. Man kann jedoch zeigen:
\begin{enumerate}
\item Eigenwerte hängen stetig von den Matrixelementen ab.
\item Eigenvektoren können bei mehrfachen Eigenwerten unstetig von den Matrixelementen abhängen.
\end{enumerate}

Die folgenden Sätze zeigen die stetige Abhängigkeit der Eigenwerte einer Matrix von ihren Matrixelementen und liefern eine obere Schranke für die Störung der Eigenwerte.

\begin{mysatz}(Elsner)

Die Eigenwerte von $A \in \C^{n \times n}$ seien $\lambda_1, \dots , \lambda_n$ und ferner seien $\mu_1, \dots , \mu_n$ die Eigenwerte der gestörten Matrix $A+E$.

Dann gibt es Permutationen $i=\left\{ 1,\dots,n \right\} \mapsto \left\{ 1,\dots,n \right\}$, so dass
\begin{align*}
|\mu_{i(j)} - \lambda_j| \leq 4\left( \|A\|_2 + \|A+E\|_2 \right)^{1-\frac{1}{n}} \|E\|_2^{\frac{1}{n}}
\end{align*}
\end{mysatz}

% subsection Störungsempfindlichkeit des Eigenwertproblems (end)

%TODO zwischenraum füllen

\begin{mysatz} (Bauer-Fike, 1960)

Ist $A$ diagonalisierbar, also
\[
AX = X\Lambda \qquad \text{ mit } \Lambda = \text{diag}(\lambda_1,\dots,\lambda_n)
\]
Sei $\mu \in \sigma(A+E)$ ein Eigenwert der gestörten Matrix $A+E$. Dann gilt
\[
\min\limits_{\lambda \in \sigma(A)} |\mu - \lambda| \leq \mathcal{K}_p(X) \|E\|_p
\]
für die $p$-Normen. Dabei bezeichnet $\mathcal{K}_p(X) = \|X\|_p \|X^{-1}\|_p$ die Kondition bzgl. der $p$-Norm oder $p$-Kondition.

Beweis: Numerische Mathematik II.
\end{mysatz}

Für symmetrische Matrizen formuliert Satz 7.11 %TODO ref fixen!
eine elementar zu beweisende Aussage. Dafür benötigen wir die folgende Fehlerabschätzung.

\begin{mysatz}
Sei $A \in \mathbb{R}^{n \times n}$ symmetrisch und sei $0 \neq x \in \mathbb{R}$ mit $\|x\|_2 = 1$ und $\mu \in \mathbb{R}$. Dann gibt es einen Eigenwert 
$\lambda \in \sigma(A)$ mit
\[
\mu - \lambda \leq \|r\|_2
\]
mit dem Residuum $r = Ax - \mu x$.

\textbf{Beweis:} Sei $\mu \notin \sigma(A)$ sonst trivial.
\begin{align*}
1 = \|x\|_2 & = \|(A -\mu I)^{-1} (A - \mu I)x \|_2 \\
 & \leq \|(A - \mu I)^{-1}\|_2 \underbrace{\|(A - \mu I)\|_2}_{\|r\|_2} = \frac{1}{\min\limits_{\lambda \in \sigma(A) |\mu - \lambda|}} \|r\|_2
\end{align*}
\end{mysatz}

%TODO Zwischenraum füllen

\begin{mysatz}
(stabiler Formelsatz zur Berechnung von $c,s$)

Ist $a_{kl} \neq 0$, so setze
\[
\rho = \frac{a_{ll}-a{kk}}{2a_{kl}}
\]
Weiter sei (Rutisheimer 1971)
\begin{align*}
t = 
\begin{cases}
\frac{1}{\rho + \sqrt{1+\rho^2}} & \rho \geq 0\\
\frac{1}{\rho - \sqrt{1+\rho^2}} & \rho < 0\\
\end{cases}
\end{align*}
\end{mysatz}


% section Eigenwertprobleme (end)

\end{document}
