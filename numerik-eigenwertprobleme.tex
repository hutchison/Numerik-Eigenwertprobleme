\documentclass[%
a4paper,
%empty,		% keine Seitenzahlen
%a5paper,	% alle weiteren Papierformat einstellbar
11pt,		% Schriftgröße (12pt, 11pt (Standard))
%leqno,		% Nummerierung von Gleichungen links
%fleqn,		% Ausgabe von Gleichungen linksbündig
]
{scrartcl}

%% Deutsche Anpassungen 
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

%obligatorischer Mathekram:
\usepackage{amssymb,amstext,dsfont,trsym,pifont}
\usepackage[sumlimits]{amsmath}
\usepackage{eulervm}

%nützlicher Mathekram:
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\off}{\text{off}}

%nützliche Extras:
\usepackage{array,
hhline,
longtable,
tabularx,
enumerate,
%hyperref,
color,
setspace,
booktabs,
%cite,
caption,
lineno,
%lastpage,
%algorithm,
ulem,
}
\usepackage{arydshln}

\usepackage[amsmath,thmmarks,thref]{ntheorem}

% meine Theoremdefinitionen:
% +------------------------+

% Definitionen:
\theoremstyle{plain}
\theoremheaderfont{\bfseries}
\theorembodyfont{}
\theoremseparator{\ }
%\theoremprework{\hfill \rule{0.5\textwidth}{1pt} \hspace*{0.25\textwidth} }
%\theorempostwork{\rule}
%\theoremindent2ex
\newtheorem{mydef}{Definition}[section]

%Sätze:
\theoremstyle{plain}
\theoremheaderfont{\bfseries}
\theorembodyfont{}
\theoremsymbol{$\Box$}
\newtheorem{mysatz}[mydef]{Satz}

%Bemerkungen:
\theoremstyle{plain}
\theoremheaderfont{\itshape}
\theorembodyfont{}
\theoremsymbol{}
\newtheorem{mybem}[mydef]{Bemerkung}

%Beispiele:
\theoremstyle{plain}
\theoremheaderfont{\itshape}
\theorembodyfont{}
\theoremsymbol{}
\newtheorem{mybsp}[mydef]{Beispiel}

% Randverwaltung (entweder geometry oder =fullpage=)
%\usepackage[left=1cm,right=1cm,top=1cm,bottom=1cm,includeheadfoot]{geometry}
\usepackage[%cm,
%headings,
]{fullpage}

% die fancy-Header:
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}

\fancyhead{}
\fancyfoot{}
%\fancyhead[L]{}
%\fancyhead[C]{}
%\fancyhead[R]{\nouppercase \leftmark}
%\fancyfoot[L]{}
\fancyfoot[C]{\thepage}
%\fancyfoot[OR]{\thepage}
%\fancyfoot[LE]{\thepage}
%Linie oben/unten
\renewcommand{\headrulewidth}{0.0pt}
\renewcommand{\footrulewidth}{0.0pt}

%kein Einrücken der Paragraphen
\parindent 0pt

%% Packages für Grafiken & Abbildungen
%\usepackage{graphicx}
%\usepackage{subfig}    %%Teilabbildungen in einer Abbildung
%\usepackage{tikz}      %%TeX ist kein Zeichenprogramm
%\usepackage[all]{xy}
\usepackage{pst-all}

\begin{document}

\setcounter{section}{6}

\section{Eigenwertprobleme}
\label{sec:Eigenwertprobleme}

\textbf{Motivation:} Mechanische Eigenschwingungen, z.B. Saite oder Membran.

Bewegungsgleichung ist die Wellengleichung mit der Auslenkung $u(x,t)$ an der Stelle $x$ zum Zeitpunkt $t$.\\

1-D Saite:
\begin{align*}
\frac{\partial^2 u}{\partial t^2} = C \cdot \frac{\partial^2 u}{\partial x^2}
\end{align*}
2-D Membran:
\begin{align*}
\frac{\partial^2 u}{\partial t^2} = C \triangle u - C \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right)
\end{align*}
jeweils mit Randberechnung wie $\left. u \right|_{\text{Rand}}=0$.\\

\textbf{Diskretisierung:}
\begin{align*}
  y''(t) & = -Ay(t) \qquad\qquad  A \in \mathbb{R}^{n \times n} \text{ spd., } y(t) \in \mathbb{R}^n\\
  \intertext{\textbf{stationäre Lösung:}}
  \text{Ansatz: } y(t) & = e^{i \omega t} y \qquad \qquad y \in \mathbb{R}^n, \omega \text{ Eigenfrequenz}\\
  \Rightarrow e^{i \omega t} (i \omega)^2 y & = -A e^{i \omega t} y\\
  \Rightarrow Ay & = \omega^2 y = \lambda y \text{ mit } \lambda = \omega^2
\end{align*}

\subsection{Grundlagen} % (fold)
\label{sub:Grundlagen}

\begin{mydef}
Sei $A \in \mathbb{C}^{n \times n}, x \in \mathbb{C}^n \backslash \left\{ 0 \right\}, \lambda \in \mathbb{C}$ und $Ax = \lambda x$. Dann heißt $x$ ein (Rechts-)\textbf{Eigenvektor} (EV) von $A$ zum \textbf{Eigenwert} (EW) $\lambda$ von $A$. Das Paar $(\lambda, x)$ heißt ein Eigenpaar von $A$. Die Menge aller Eigenwerte von $A$
\begin{align*}
\sigma(A) := \left\{ \lambda \ | \ \lambda \text{ ist Eigenwert von } A \right\}
\end{align*}
heißt das \textbf{Spektrum} von $A$.
\end{mydef}
\textit{Bemerkung:}\\[-1.3cm] %rrrrreal dirrty (but who cares anyway when it looks good?!)
\begin{align*}
Ax = \lambda x & \Leftrightarrow (\lambda I - A)x = 0 \\
 & \Leftrightarrow \det(\lambda I - A)=0\\
 & \Leftrightarrow \lambda \text{ ist Nullstelle des charakteristischen Polynoms.}
\end{align*}
Ist $x$ ein Eigenvektor, so auch $\theta x$ mit $\theta \neq 0$. Deshalb werden Eigenvektoren oft normiert, d.h. $\| x \|_2=1$.\\

\begin{mysatz}
Das charakteristische Polynom $p_A(\lambda) = \det(\lambda I - A)$ von $A$ besitzt die eindeutige Faktorisierung
\begin{align*}
p_A(\lambda) = (\lambda - \lambda_1)^{m_1} \cdot (\lambda - \lambda_2)^{m_2} \cdot \ldots \cdot (\lambda - \lambda_k)^{n_k}
\end{align*}
mit den $k$ paarweise verschiedenen Eigenwerten $\lambda_k$ von $A$ mit $m_1 + \ldots + m_k = n$.

Dabei ist $m_i$ die algebraische Vielfachheit von $\lambda_i$.\newline

\textbf{Beweis:} Fundamentalsatz der Algebra.
\end{mysatz}

\begin{mydef}
  Die \textbf{geometrische Vielfachheit} eines Eigenwertes $\lambda$ von $A$ ist die Dimension des \textbf{Nullraumes} von $\lambda I -A$.
\end{mydef}

%\newpage

\begin{mybem}
Die geometrische Vielfachheit ist stets kleiner oder gleich der algebraischen Vielfachheit.
So besitzt der \textbf{Jordan-Block}\footnote{benannt nach Marie Ennemond Camille Jordan (5. Januar 1838 - 21. Januar 1922), französischer Mathematiker}
\begin{align*}
J_2(0) = 
\begin{pmatrix}
0 & 1\\
0 & 0
\end{pmatrix}
\end{align*}
das charakteristische Polynom $\lambda^2$, so dass 0 ein Eigenwert der algebraischen Vielfachheit 2 ist. Jedoch wird der Nullraum von $OI - J_2(0) = \begin{pmatrix}
0 & -1\\
0 & 0
\end{pmatrix}$
vom Einheitsvektor $e_1$ ausgespannt.
Die geometrische Vielfachheit des Eigenwertes 0 ist also nur 1.

Allgemein gilt: Jordan-Blöcke der Dimension größer 1 besitzen kein vollständiges System aus Eigenvektoren (die Matrix besitzt einen \textit{Defekt}).
Die Jordansche Normalform lässt sich numerisch nicht berechnen, denn die Jordan-Blöcke reagieren extrem empfindlich auf Störungen (siehe Beispiel \thref{bspEWJordanBlock})
\end{mybem}

\textbf{Erinnerung:}

Seien $A,X \in \mathbb{R}^{n \times n}$ und sei $X$ nicht-singulär. Dann ist $B=X^{-1}AX$ \textbf{ähnlich} zu $A$, die Abbildung heißt \textbf{Ähnlichkeitstransformation}.

Wir suchen Ähnlichkeitstransformationen der Gestalt
\begin{align*}
X^{-1}AX = \Lambda
\end{align*}
die $A$ auf Diagonalgestalt transformiert.

Die Spalten von $X$ sind dann die Eigenvektoren, die Diagonalelemente von $\Lambda$ die Eigenwerte.

Besonders vorteilhaft sind Ähnlichkeitstransformationen durch unitäre (orthogonale) Matrizen. Eine unitäre $n \times n$ Matrix besitzt $n^2$ Matrixelemente, die jedoch $\frac{n(n-1)}{2}$ Orthogonalitätsbedingungen und $n$ Normierungsbedingungen erfüllen.
Dann verbleiben $n^2 - \frac{n(n-1)}{2} -n = \frac{n(n-1)}{2}$ Freiheitsgrade. 

Durch geeignete Nutzung der Freiheitsgrade erscheint es möglich, eine gegebene $n \times n$ Matrix auf Dreiecksgestalt zu transformieren, denn eine Dreiecksmatrix besitzt $\frac{n(n-1)}{2}$ Nichtnullelemente.  

\begin{mysatz}
(Schursche Normalform (auch Schur-Zerlegung), Schur\footnote{Issai Schur (10. Januar 1875 - 10. Januar 1941), hauptsächlich in Deutschland arbeitender Mathematiker} 1909)


Zu jeder komplexen Matrix $A \in \mathbb{C}^{n \times n}$ existiert eine unitäre Matrix $U \in \mathbb{C}^{n \times n}$, eine Diagonalmatrix 
\begin{align*}
D = \text{diag}(\lambda_1, \ldots, \lambda_n)
\end{align*}
und eine (echte) obere Dreiecksmatrix mit verschwindenen Diagonalelementen, so dass 
\begin{align*}
 & U^H A U = D + N \\
 \text{oder} \qquad & A = U(D+N)U^H
\end{align*}
(Dabei ist $U^H = \overline{U}^T$, die zu $U$ \uline{adjungierte} Matrix; $U$ unitär: $\Leftrightarrow U^HU = I$.)\\

\textit{Bemerkung:}
\begin{enumerate}
    \item Der Satz von Schur ist die natürliche Verallgemeinerung des Spektralsatzes für symmetrische Matrizen; Beweis ebenso induktiv.
    \item Die Diagonalelemente von $D$ sind die Eigenwerte von $A$.
\end{enumerate}
\textbf{Beweis:} Induktionsanfang:

Gemäß dem Fundamentalsatz der Algebra besitzt $A \in \mathbb{C}^{n \times n}$ einen Eigenwert $\lambda_1 \in \mathbb{C}$.

Es gilt $Au = \lambda_1 u$ für ein $u \in \mathbb{C}^n$ mit $\| u \|_2 = 1$.

Ergänze $u$ durch $u_2, \ldots, u_n$ zu einer Orthogonalbasis des $\mathbb{C}^n$ und sei
\begin{align*}
U_1 & = \left[ u, u_2, \ldots, u_n \right] \in \mathbb{C}^{n \times n}
\intertext{Dann gilt}
AU_1 & = \left( \lambda_1 u, Au_2, \ldots, Au_n \right)
\intertext{und weiter}
U_1^H A U_1 & =
\left(
\begin{array}{c:ccc}
\lambda_1 & * & \dots & *\\\hdashline
0 & * & \dots & *\\
\vdots & \vdots & & \vdots\\
0 & * & \dots &  *
\end{array}
\right)
\begin{array}{c}
1\\
\\
n-1\\
\\
\end{array}\\
& \phantom{=} 
\begin{array}{ccccc}
  & \hspace{0.8em} 1 & & n-1 & 
\end{array}
\end{align*}
Induktionsschluss: Argumentiere wie oben auf dem rechten unteren $(n-1) \times (n-1)$ Block

\begin{center}
%+-+-+-+-+
% Jap, das wird als Bild eingefügt, da man sonst nicht so eine schöne Box um die rechte untere
% Teilmatrix malen kann. So sieht's aber echt schnieke aus :)
%+-+-+-+-+
\begin{pspicture}(10,4.5)

% zum justieren:
%\psgrid(0,0)(10,5)

\put(0,2.5){
$
U_i^H \dots U_1^H A U_1 \dots U_i =
\begin{pmatrix}
\lambda_1   & *         & \cdots    & \cdots    & \cdots    & *\\
0           & \ddots    & \ddots    &           &           & \vdots\\
\vdots      & \ddots    & \lambda_i & *         & \cdots    & *\\
\vdots      &           & 0         & *         & \cdots    & *\\
\vdots      &           & \vdots    & \vdots    &           & \vdots\\
0           & \cdots    & 0         & *         & \cdots    & *
\end{pmatrix}
$
}
% Die Box um die Teilmatrix:
\psframe(6.9,2.3)(8.7,0.85)
\end{pspicture}
\end{center}
mit der Blockmatrix
\begin{align*}
U_i & = 
\begin{pmatrix}
1 				& 			&	& \text{0}\\
 				& \ddots 	& 	& \\
 				&			& 1\\
\text{0} 	& 			&  	& \Large\boxed{\^U_i} 
\end{pmatrix}
\qquad
\^U_i \text{ unitär}
\end{align*}
\
\end{mysatz}

Für normale Matrizen lässt sich mehr zeigen:
\begin{align*}
A \text{ normal} \Leftrightarrow A^HA = AA^H
\end{align*}
Zu den normalen Matrizen zählen die \textbf{hermiteschen/selbstadjungierten} Matrizen (d.h. $A^H = A$) und die reellen symmetrischen Matrizen.

\begin{mysatz} (Hermite\footnote{Charles Hermite (24. Dezember 1822 - 14. Januar 1901), französischer Mathematiker} 1850 - \textit{Spektralsatz})

Ist $A \in \C^{n \times n}$ normal, so gibt es eine unitäre Matrix $U$ mit
\begin{align*}
U^H A U = 
\begin{pmatrix}
\lambda_1 & & 0\\
& \ddots & \\
0 & & \lambda_n
\end{pmatrix}
=
\text{diag}(\lambda_1,\dots,\lambda_n)
\end{align*}

%\newpage

\textbf{Beweis:}

Schursche Normalform
\begin{align*}
U^H A U = 
\begin{pmatrix}
\lambda_1 & & *\\
& \ddots & \\
0 & & \lambda_n
\end{pmatrix}
=: R
\end{align*}
Mit $A$ ist auch $R$ normal, denn
\begin{align*}
RR^H & = U^H A \underbrace{(U \cdot U^H)}_{I} A^H U = U^H A^H A U\\
 & = (U^H A^H U) U^H A U  = R^H R
\end{align*}
In Komponenten
\begin{align*}
R R^H & = 
\begin{pmatrix}
\lambda_1   & r_{12}    & \dots     & r_{1n}\\
            & \ddots    & \ddots    & \vdots\\
            &           & \ddots    & r_{n-1,n}\\
0           &           &           & \lambda_n
\end{pmatrix}
\begin{pmatrix}
\overline{\lambda_1}    & & & 0\\
\overline{r_{12}}       & \ddots\\
\vdots                  & \ddots & \ddots\\
\overline{r_{1n}}       & \dots & \dots & \overline{\lambda_n}
\end{pmatrix}\\
& = 
\begin{pmatrix}
\overline{\lambda_1}    & & & 0\\
\overline{r_{12}}       & \ddots\\
\vdots                  & \ddots & \ddots\\
\overline{r_{1n}}       & \dots & \dots & \overline{\lambda_n}
\end{pmatrix}
\begin{pmatrix}
\lambda_1   & r_{12}    & \dots     & r_{1n}\\
            & \ddots    & \ddots    & \vdots\\
            &           & \ddots    & r_{n-1,n}\\
0           &           &           & \lambda_n
\end{pmatrix}
= R^H R
\end{align*}
Also gilt komponentenweise:
\begin{align*}
\text{für } i=j=1: \qquad & |\lambda_1|^2 + |r_{12}|^2 + \dots + |r_{1n}|^2 = |\lambda_1|^2\\
& \Rightarrow r_{12} = r_{13} = \dots = r_{1n} = 0\\
\text{für } i=j=2: \qquad & |\lambda_2|^2 + |r_{23}|^2 + \dots + |r_{2n}|^2 = |\lambda_2|^2\\
& \Rightarrow r_{23} = \dots = r_{2n} = 0 \qquad \text{ usw. usf.}
\end{align*}
\
\end{mysatz}

\subsection{Störungsempfindlichkeit des Eigenwertproblems}
\label{sub:Störungsempfindlichkeit des Eigenwertproblems}

Die Störungsempfindlichkeit der Lösung eines LGS hängt wesentlich von der Konditionszahl
\begin{align*}
  \mathcal{K}(A) & = \|A\| \cdot \|A^{-1}\| \qquad \text{ab.}
\end{align*}
Für Eigenwertprobleme sind die Zusammenhänge komplexer, denn die Diagonalisierbarkeit (Fehlen eines Defekts) ist entscheidend.
\begin{mybsp}\label{bspEWJordanBlock} Der Jordan-Block 
$A = 
\begin{pmatrix}
2 & 1\\
0 & 2
\end{pmatrix}
$
ist nicht diagonalisierbar. 

Für $\varepsilon > 0$ sei
$A(\varepsilon) = 
\begin{pmatrix}
2 & 1\\
\varepsilon & 2
\end{pmatrix}
$
mit den Eigenwerten $\lambda_1(\varepsilon)=2+\sqrt{2},\ \lambda_2(\varepsilon)=2-\sqrt{2}$.

Es gilt
\begin{align*}
\frac{d\lambda_1(\varepsilon)}{d\varepsilon} & = \frac{1}{2\sqrt{\varepsilon}} \qquad \frac{d\lambda_2(\varepsilon)}{d\varepsilon} = -\frac{1}{2\sqrt{\varepsilon}}
\end{align*}
also
\begin{align*}
\lim\limits_{\varepsilon\to0} & \left| \frac{d \lambda_i(\varepsilon)}{d \varepsilon} \right| = \infty
\end{align*}
\center $\Rightarrow$ \textbf{unbeschränkte Störungsempfindlichkeit!}
\end{mybsp}

Beispiel \ref{bspEWJordanBlock} zeigt, dass Eigenwerte nicht stetig differenzierbar von den Matrixelementen abhängen können. Man kann jedoch zeigen:
\begin{enumerate}
\item Eigenwerte hängen stetig von den Matrixelementen ab.
\item Eigenvektoren können bei mehrfachen Eigenwerten unstetig von den Matrixelementen abhängen.
\end{enumerate}

Die folgenden Sätze zeigen die stetige Abhängigkeit der Eigenwerte einer Matrix von ihren Matrixelementen und liefern eine obere Schranke für die Störung der Eigenwerte.

\begin{mysatz}(Elsner\footnote{Ludwig Elsner})

Die Eigenwerte von $A \in \C^{n \times n}$ seien $\lambda_1, \dots , \lambda_n$ und ferner seien $\mu_1, \dots , \mu_n$ die Eigenwerte der gestörten Matrix $A+E$.

Dann gibt es eine Permutation $i=\left\{ 1,\dots,n \right\} \mapsto \left\{ 1,\dots,n \right\}$, so dass
\begin{align*}
|\mu_{i(j)} - \lambda_j| \leq 4\left( \|A\|_2 + \|A+E\|_2 \right)^{1-\frac{1}{n}} \|E\|_2^{\frac{1}{n}}
\end{align*}
\
\end{mysatz}

\begin{mysatz} (Bauer\footnote{Friedrich Ludwig Bauer (10. Juni 1924), deutscher Mathematiker und Pionier der Informatik}-Fike, 1960)

Ist $A$ diagonalisierbar, also
\begin{align*}
AX = X\Lambda \qquad \text{ mit } \Lambda = \text{diag}(\lambda_1,\dots,\lambda_n)
\end{align*}
Sei $\mu \in \sigma(A+E)$ ein Eigenwert der gestörten Matrix $A+E$. Dann gilt
\begin{align*}
\min\limits_{\lambda \in \sigma(A)} |\mu - \lambda| \leq \mathcal{K}_p(X) \|E\|_p
\end{align*}
für die $p$-Normen. Dabei bezeichnet $\mathcal{K}_p(X) = \|X\|_p \|X^{-1}\|_p$ die Kondition bzgl. der $p$-Norm oder $p$-Kondition.\\

\textbf{Beweis:} Numerische Mathematik II.
\end{mysatz}

Für symmetrische Matrizen formuliert Satz \thref{satz:ewintervall} eine elementar zu beweisende Aussage. Dafür benötigen wir die folgende Fehlerabschätzung.

\begin{mysatz}\label{satz:ewresiduum}
Sei $A \in \mathbb{R}^{n \times n}$ symmetrisch und sei $0 \neq x \in \mathbb{R}$ mit $\|x\|_2 = 1$ und $\mu \in \mathbb{R}$. Dann gibt es einen Eigenwert 
$\lambda \in \sigma(A)$ mit
\begin{align*}
|\mu - \lambda| \leq \|r\|_2
\end{align*}
mit dem Residuum $r = Ax - \mu x$.\\

\textbf{Beweis:} Sei $\mu \notin \sigma(A)$ sonst trivial.
\begin{align*}
1 = \|x\|_2 & = \|(A -\mu I)^{-1} \cdot  (A - \mu I)x \|_2 \\
& \leq \|(A - \mu I)^{-1}\|_2 \cdot  \underbrace{\|(A - \mu I)\|_2}_{\|r\|_2} = \frac{1}{\min\limits_{\lambda \in \sigma(A)} |\mu - \lambda|} \|r\|_2
\end{align*}
\
\end{mysatz}

\begin{mysatz}\label{satz:ewintervall}
Seien $A, A+E \in \R^{n \times n}$ symmetrische Matrizen mit
\begin{align*}
\| E \|_F \leq E
\end{align*}
Dann enthält jedes Intervall
\begin{align*}
\lambda - \varepsilon \leq \mu \leq \lambda + \varepsilon
\end{align*}
um einen Eigenwert $\lambda$ von $A$ auch einen Eigenwert $\mu$ von $A+E$ und umgekehrt.\\

\textbf{Beweis:} Sei $Ax=\lambda x$ und $x \neq 0$. Dann gilt
\begin{align*}
\| (A+E)x - \lambda x \|_2 = \| (A+E)x - Ax \|_2 \leq \|E\|_2 \cdot \|x||_2 \leq \|E\|_F \cdot \|x\|_2
\end{align*}
Nach Satz \thref{satz:ewresiduum} gibt es einen Eigenwert $\mu$ von $A+E$ in einer $\varepsilon$-Umgebung von $\lambda$.

Umkehrung durch Vertauschen von $A$ und $A+E$.
\end{mysatz}

\newpage

\begin{mysatz}(Satz von Gerschgorin\footnote{Semjon Aranowitsch Gerschgorin (24. August 1901 - 30. Mai 1933), weißrussisch-sowjetischer Mathematiker})

Sei $A \in \C^{n \times n}$ und seien komplexe Kreisscheiben \hfill (\textit{Gerschgorin-Kreise})
\begin{align*}
  R_i = \left\{ z \in \C; |z-a_{ii}| \leq \sum\limits_{\substack{i=1\\ i \neq j}}^n |a_{ij}| \right\}
\end{align*}
definiert. Dann gilt
\begin{align*}
\sigma(A) \subseteq \bigcup\limits_{i=1}^n R_i
\end{align*}

\textbf{Beweis:}
Sei $(\lambda,x)$ ein beliebiges Eigenpaar von $A$. Sei $x_i$ eine Komponente größten Betrags, so dass\\
$\forall j=1\dots n: |x_j| \leq |x_i| $. O.B.d.A. sei $x_i=1$.

Dann gilt für die $i$-te Zeile von $Ax = \lambda x$
\begin{align*}
  a_{ii} x_i + \sum\limits_{j \neq i}^n a_{ij} x_i = \lambda \cdot x_i
\end{align*}
oder gleichwertig
\begin{align*}
  \lambda - a_{ii} = \sum\limits_{j \neq i}^n a_{ij} x_j
\end{align*}
Wegen $|x_j| \leq 1$ folgt
\begin{align*}
  |\lambda - a_{ii}| \leq \sum\limits_{j \neq i}^n |a_{ij}|
\end{align*}
so dass $\lambda \in \R$.
\end{mysatz}

%\newpage

\subsection{Jacobi-Verfahren}
\label{sub:Jacobi-Verfahren}

Ältestes numerisches Verfahren zur Lösung des symmetrischen Eigenwertproblems. Es wurde mit Jahr 1846 von Jacobi\footnote{Carl Gustav Jacob Jacobi (10. Dezember 1804 - 18. Februar 1851), deutscher Mathematiker} vorgeschlagen (Crelles Journal\footnote{\textit{Journal für die reine und angewandte Mathematik.} Gegr. im Jahre 1826 von August Leopold Crelle, de Gruyter, Berlin}) und zur Berechnung der Stabilität von Planetenbahnen einsetzt (ein $7 \times 7$ Eigenwertproblem gemäß den damals 7 bekannten Planeten).\\

\uline{Ziel des Jacobi-Verfahrens:}
\begin{quote}
Berechnung aller Eigenwerte (und ggf. auch Eigenvektoren) einer kleinen (dicht besetzten) symmetrischen Matrix.
\end{quote}

%\newpage

\uline{Idee:}
\begin{quote}
Transformiere die symmetrische Matrix $A$ durch eine Folge von Ähnlichkeitstransformationen auf Diagonalgestalt. 
Praktisch wird die Folge jedoch abgebrochen, falls der \textbf{Außendiagonalanteil}
\begin{align*}
  \off(B) := \sum\limits_{\substack{i,j=1\\ i \neq j}}^n b_{ij}^2
\end{align*}
der transformierten Matrix $B$ hinreichend klein ist.

Die Eigenwerte von $B$ stimmen mit den Eigenwerten von $A$ überein. Gilt $\off(B) \leq \varepsilon^2$, so sind nach Satz \thref{satz:ewintervall}
die Diagonalelemente von $B$ bis auf einen Fehler $\leq \varepsilon$ gleich den gesuchten Eigenwerten von $A$.
\end{quote}

\textbf{Ähnlichkeitstransformation:}
\begin{align*}
A \rightarrow B = J(k,l,\theta)^T \cdot A \cdot J(k,l,\theta)
\end{align*}
mit den \uline{Jacobi-Rotationen} (Givens\footnote{James Wallace Givens, Jr. (14. Dezember 1910 - 5. März 1993), US-Amerikanischer Mathematiker und Pionier der Informatik} Rotationen)
\begin{align*}
J(k,l,\theta) = & 
\left(
\begin{array}{ccccccccccc}
1   &       &       &       &       &       &       &       &       &       & 0\\
    &\ddots \\
    &       & 1\\
    &       &       & c     &       &       &       & s\\
    &       &       &       & 1\\
    &       &       &       &       &\ddots\\
    &       &       &       &       &       & 1\\
    &       &       & -s    &       &       &       & c\\
    &       &       &       &       &       &       &       & 1\\
    &       &       &       &       &       &       &       &       &\ddots\\
0   &       &       &       &       &       &       &       &       &       & 1\\
\end{array}
\right)
\begin{array}{c}
 \\ \\ \\ k\\ \\ \\ \\ l\\ \\ \\ \\
\end{array}
\qquad
\begin{array}{crl}
\text{für} & k,l & \leq n\\
 & c & = cos\ \theta\\
 & s & = sin\ \theta
\end{array}\\
&
\hspace*{4ex}
\begin{array}{ccccccccccc}
  \phantom{0} & \phantom{\dots} & \phantom{1} & k & \phantom{1} & \phantom{\dots} & \phantom{1} & l
\end{array}
\end{align*}
Die Jacobi-Rotationen sind orthogonale Matrizen/Ebenendrehungen in der $(x_k,x_l)$ Ebene um den Winkel $\theta$.

Die Abbildung $A \rightarrow B = J^T A J$ modifiziert nur die $k$-te und $l$-te Zeile sowie Spalte! Nur $4n-4$ Matrixelemente sind betroffen; \uline{Aufwand $O(n)$.}

%\newpage

\begin{mysatz}
Es gilt
\begin{align*}
  \off(B) = \off(A) - 2 a_{kl}^2 + 2b_{kl}^2 
\end{align*}
\textbf{Beweis:}
Orthogonale Abbildungen erhalten die Euklidische Länge von Vektoren. Dies zeigt die orthogonale Invarianz der Frobeniusnorm im Sinne von
\begin{align*}
\|UAV\|_F = \|A\|_F
\end{align*}
für alle orthogonalen $U,V \in \R^{n \times n}$.

Man erhält für die Jacobi-Transformation
\begin{align*}
  \off(B)   & = \|B\|_F^2 - \sum\limits_{\substack{i=1\\ i \neq k,l}}^n b_{ii}^2 - \left( b_{kk}^2 + b_{ll}^2 \right)\\
            & = \|A\|_F^2 - \sum\limits_{\substack{i=1\\ i \neq k,l}}^n a_{ii}^2 - \left( b_{kk}^2 + b_{ll}^2 \right)\\
            & = \off(A) + (a_{kk}^2 + a_{ll}^2) - (b_{kk}^2+b_{ll}^2)\label{eq:eq1}\tag{1}
\end{align*}
Weiter gilt
\begin{align*}
\begin{pmatrix}
b_{kk} & b_{kl}\\
b_{lk} & b_{ll}
\end{pmatrix}
& = 
\begin{pmatrix}
c & s\\
-s & c
\end{pmatrix}^T
\begin{pmatrix}
a_{kk} & a_{kl}\\
a_{lk} & a_{ll}
\end{pmatrix}
\begin{pmatrix}
c & s\\
-s & c
\end{pmatrix}
\end{align*}
sodann wiederum wegen der orthogonalen Invarianz der Frobeniusnorm zusammen mit der Symmetrie von $A$ und $B$
\begin{align*}
b_{kk}^2 + b_{ll}^2 + 2b_{kl}^2 & = a_{kk}^2 + a_{ll}^2 + 2a_{kl}^2
\end{align*}
folgt einesetzt in \eqref{eq:eq1} ergibt sich
\begin{align*}
\off(B) & = \off(A) - 2a_{kl}^2 + 2b_{kl}^2
\end{align*}
\
\end{mysatz}

\uline{Ziel:} Durch Wahl des Drehwinkels $\theta$ erreiche $b_{kl}=0$ und damit die \textbf{größtmögliche Reduktion} von $\off(A)$.

\begin{mysatz}
(stabiler Formelsatz zur Berechnung von $c,s$)

Ist $a_{kl} \neq 0$, so setze
\begin{align*}
\rho = \frac{a_{ll}-a{kk}}{2a_{kl}}
\end{align*}
Weiter sei (Rutishauser\footnote{Heinz Rutishauser (30. Januar 1918 - 10. November 1970), Schweizer Mathematiker und Pionier der modernen numerischen Mathematik und Informatik}, 1971)
 
\begin{align*}
t = 
\begin{cases}
\frac{1}{\rho + \sqrt{1+\rho^2}} & \rho \geq 0\\
\frac{1}{\rho - \sqrt{1+\rho^2}} & \rho < 0\\
\end{cases}
\end{align*}
und $c = \frac{1}{\sqrt{1+t^2}}, s=t \cdot  c$.\\

Es gilt $c^2 + s^2 = 1$, also $c = \cos \theta$ und  $s = \sin \theta$ für ein $\theta \in \left[0,2 \pi \right]$ und außerdem $b_{kl} = 0$.

\textbf{Beweis:}
Es gilt
\begin{align*}
\begin{pmatrix}
b_{kk} & b_{kl}\\
b_{lk} & b_{ll}
\end{pmatrix}
& = 
\begin{pmatrix}
c & s\\
-s & c
\end{pmatrix}^T
\begin{pmatrix}
a_{kk} & a_{kl}\\
a_{lk} & a_{ll}
\end{pmatrix}
\begin{pmatrix}
c & s\\
-s & c
\end{pmatrix}
\intertext{und wegen $a_{kl} = a_{lk}$ gilt falls $a_{kl} \neq 0$}
b_{kl} & = a_{kl}(c^2 - s^2) + (a_{kk} - a_{ll})cs\\
& = 2 c s a_{kl} \underbrace{\left( \frac{c^2 - s^2}{2cs} + \frac{a_{kk} - a_{ll}}{2a_{kl}} \right)}_{=0}
\intertext{Um $b_{kl}=0$ zu erreichen, muss also gelten}
\frac{c^2-s^2}{2cs} & = \frac{ \frac{1}{1+t^2} - \frac{t^2}{1+t^2} }{2 \frac{t}{\sqrt{1+t^2}} \cdot \frac{1}{\sqrt{1+t^2}}} = \frac{1-t^2}{2t} = \rho
\end{align*}
Also muss $t$ die Lösung der quadratischen Gleichung
\begin{align*}
t^2 + 2 \rho t - 1 = 0
\end{align*}
sein. Durch Einsetzen zeigt man sofort, dass $t$ in beiden Fällen $(\rho \geq 0 \text{ und } \rho < 0)$ die Gleichung erfüllt. Schließlich gilt
\begin{align*}
c^2 + s^2 = \frac{1}{1+t^2} + \frac{t^2}{1+t^2} = 1
\end{align*}
\
\end{mysatz}

\textit{Bemerkung:} Die Formeln von Satz 7.14 %TODO ref fixen
für $c$ und $s$ erfordern nicht die teure Auswertung trigonometrischer Funktionen und sind im Hinblick auf Auslöschungseffekte stabil.

%Fett:
Varianten des Jacobi Verfahrens:
\begin{enumerate}
\item Klassisches Jacobi-Verfahren
  %TODO: ordentlich machen:
\begin{verbatim}
while off(A) > eps^2
    bestimme Indizes k,l mit |a_{kl}| = max_{i,j}_{i \neq j} |a_{i,j}|
    ersetze A durch J^T A J mit J = J(k,l,\theta)
end.
\end{verbatim}
$\to$ erfordert aufwändigen Suchprozess!

Konvergenzverhalten: (Schönhagen 1964)

Asymptotisch (für große $i$) \uline{quadratisch}

\uline{Konvergenz}, d.h. für hinreichend großes $i$ gibt es eine Konstante $C=C(A)$ mit 
\begin{align*}
\off(A_{i+N} \leq C(\off(A_i))^2
\end{align*}
wobei $A = A_0,A_1,A_2, \dots $ die vom Jacobi-Prozess erzeugte Matrizen bezeichnen und $N = \frac{1}{2}n(n-1)$

\item Zyklisches Jacobi-Verfahren %unterstrichen

Durchlaufe die Indexpaare $(k,l)$ in fester Reihenfolge. Dies vermeidet den Suchprozess $\leadsto$ \uline{ähnliche Konvergenzeigenschaften.}
\end{enumerate}

\begin{mysatz}(Lineare Konvergenz)

Sei $(A)_{i \in \mathbb{N}}$ mit $A_0 = A$ die vom klassischen Jacobi-Verfahren erzeugte Matrizenfolge und $N = \frac{1}{2} n (n-1)$. Dann gilt
\begin{align*}
\off(A_{i+1}) \leq \left( 1-\frac{1}{N} \right) \off(A_i)
\end{align*}
und noch einen vollen Zyklus ($N$ Schritte)
\begin{align*}
\off(A_{i+N}) \leq \frac{1}{e} \off(A_i)
\end{align*}

\textbf{Beweis:}
Betrachte $A \to J^T A J$ und leite obere Abschätzung für $\off(J^t A J)$ her.

Für das betragsgrößte Außerdiagonalelement gilt
\begin{align*}
  a_{kl}^2 = \max\limits_{i \neq j} a_{ij}^2
\end{align*}
Also gilt
\begin{align*}
\off(A) = \sum\limits_{i \neq j} a_{ij}^2 \leq 2 N \max\limits_{i \neq j} a_{ij}^2 = 2 N a_{kl}^2 %TODO tag als (1)
\end{align*}
da $A: n^2-n = 2N$ Außerdiagonalelemente besitzt.

Es folgt
\begin{align*}
\off(J^T A J) = \off (A) - 2a_{kl}^2 \leq %hierüber Gleichung 1
\left( 1 - \frac{1}{N} \right)\off(A)
\intertext{oder gleichwertig für alle $i$}
\off(A_{i+1} \leq \left( 1 - \frac{1}{N} \right)\off(A_i)
\intertext{Also folgt}
\off(A_{i+N}) \leq \left( 1 - \frac{1}{N} \right)^N \off (A_i)
\end{align*}
Mit $\left( 1-\frac{1}{N} \right)^N \leq e^{-1}$ folgt die Behauptung.
\end{mysatz}

\textit{Bemerkung:}
\begin{quote}
Eigenvektoren erhält man beim Jacobi-Verfahren aus dem Produkt aller planaren Drehungen
\begin{align*}
  A \to J_1^T A J_1 \to (J_1 J_2)^T A J_1 J_2 \to \dots \to \left( \prod\limits_{i=1}^k J_i \right)^T A \left( \prod\limits_{i=1}^k J_i \right)
\end{align*}
\end{quote}

\subsection{Potenzmethode} % (fold)
\label{sub:Potenzmethode}

Oft benötigt man nicht alle, sondern nur wenige (betragsmäßig extremale) Eigenwerte und die zugehörigen Eigenvektoren. Durch \textbf{Vektoriterationen} können Näherungen gewonnen werden. 

Berechne den betragsmäßig größten Eigenwert einer Matrix $A \in \R^{n \times n}$ (oder $\C^{n \times n}$) und den Eigenvektor.

Betrachte dazu die Folge $\left( y^{(k)} \right)_{k \in \mathbb{N}}$ mit 
\begin{align*}
y^{(k+1)} = Ay^{(k)}
\end{align*}
also $y^{(k)} = A^k y^{(0)}$ zu einem Startvektor $y^{(0)}$.

\begin{mysatz}

Sei $A$ diagonalisierbar und besitze die Eigenpaare $(\lambda_i, x_i)$ mit $\|x_i\| = 1$ und 
\begin{align*}
|\lambda_1| \leq |\lambda_2| \dots |\lambda_{n-1}| < |\lambda_n|
\end{align*}
Falls $y^{(0)} = \sum\limits_{i=1}^n \alpha_i x_i$ mit $\alpha_n \neq 0$, so gilt für $y^{(k)} = Ay^{(k-1)}$
\begin{enumerate}
  \item $y^{(k)} = \lambda_n^k \left( \alpha_n x_n + O(\left| \frac{\lambda_{n-1}}{\lambda_n} \right|^k \right)$
    Damit konvergiert $\frac{y^{(k)}}{\alpha_n \lambda_n^k}$ gegen den Eigenvektor $x_n$ von $A$.
\item Für den Rayleigh-Quotienten $\rho(y^{(k)}$ gilt
\begin{align*}
\rho(y^{(k)}) & := \frac{\langle y^{(k)}, Ay^{(k)} \rangle}{\langle y^{(k)}, y^{(k)} \rangle} = \lambda_n + O\left( \left| \frac{\lambda_{n-1}}{\lambda_n} \right|^k\right)
\intertext{und falls $A$ normale Matrizen}
\rho(y^{(k)}) & = \lambda_n + O \left( \left| \frac{\lambda_{n-1}}{\lambda_n} \right|^{2k} \right)
\end{align*}
\end{enumerate}
\textbf{Beweis:}
\begin{enumerate}
\item 
\begin{align*}
y^{(0)} & = \sum\limits_{i=1}^n \alpha_i x_i\\
Ay^{(0)} & = \sum\limits_{i=1}^n (\alpha_i \lambda_i) x_i\\
y^{(k)} & = A^k y^{(0)} = \sum\limits_{i=1}^n \alpha_i \lambda_i^k x_i\\
& = \lambda_n^k \left( \uline{\left( \frac{\lambda_1}{\lambda_n} \right)}^k \alpha_1 x_1 + \dots +  \uline{\left( \frac{\lambda_{n-1}}{\lambda_n} \right)}^k \alpha_{n-1} x_{n-1} + \alpha_n x_n \right)
\end{align*}
\end{enumerate}
\end{mysatz}

% subsection Potenzmethode (end)

% section Eigenwertprobleme (end)

\end{document}
